# GPT4OCR

<div align="center">
  <p>
      <img width="50%" src="images/DLVC.png"></a>
  </p>

  <a href=""><img src="https://img.shields.io/github/stars/{SCUT-DLVCLab}/{GPT4OCR}.svg"></a>
  <a href=""><img src="https://img.shields.io/github/issues/{SCUT-DLVCLab}/{GPT4OCR}.svg"></a>
  <a href=""><img src="https://img.shields.io/github/issues-pr/{SCUT-DLVCLab}/{GPT4OCR}.svg"></a>

[English](README.md) | [简体中文](README.zh-CN.md)
<br>

This repository evaluates the performance of GPT-4V(ision) on various OCR-related tasks. 

<!-- <div>
    <a href=""><img src="https://img.shields.io/badge/-Run on gradio-orange" alt="gradio"></a>
</div> -->


<h2>Scene Text Recognition</h2>
<div>
    <a href="results/str/results_str.md"><img src="https://img.shields.io/badge/Evaluation- 🗒️Results-blue" alt="results_vie"></a>
</div>

<h2>Handwritten Text Recognition</h2>
<div>
    <a href="results/htr/result_htr.md"><img src="https://img.shields.io/badge/Evaluation- 🗒️Results-blue" alt="results_vie"></a>
</div>

<h2>Handwritten Mathematical Expression Recognition</h2>
<div>
    <a href="results/hmer/results_hmer.md"><img src="https://img.shields.io/badge/Evaluation- 🗒️Results-blue" alt="results_vie"></a>
</div>

<h2>Visual Information Extraction</h2>

<div>
    <a href="results/vie/results_vie.md"><img src="https://img.shields.io/badge/Evaluation- 🗒️Results-blue" alt="results_vie"></a>
    <a href="https://github.com/SCUT-DLVCLab/Document-AI-Recommendations/tree/main"><img src="https://img.shields.io/badge/Resource-🧷Collections-orange" alt="recommendations_vie"></a>
</div>

To learn more about Visual Information Extraction, please refer to [Document-AI-Recommendations](https://github.com/SCUT-DLVCLab/Document-AI-Recommendations/tree/main).

Visual Information Extraction(VIE) aims at mining, analyzing, and extracting key fields entities contained in visually rich documents. For example, given an image of a receipt, the VIE algorithms will tell information such as store name, product details, price, etc. For documents like forms, VIE algorithms will tell the key-value pairs contained.

<h2>Table Structure Recognition</h2>
<div>
    <a href="results/tsr/results_tsr.md"><img src="https://img.shields.io/badge/Evaluation- 🗒️Results-blue" alt="results_vie"></a>
    <a href="https://github.com/SCUT-DLVCLab/Document-AI-Recommendations/tree/main"><img src="https://img.shields.io/badge/Resource-🧷Collections-orange" alt="recommendations_tsr"></a>
</div>

To learn more about Table Structure Recognition, please refer to [Document-AI-Recommendations](https://github.com/SCUT-DLVCLab/Document-AI-Recommendations/tree/main).

Table Structure Recognition(TSR) aims to recognize the cellular structures of tables from table images by extracting the coordinates of cell boxes and row/column spanning information. This task is very challenging since tables may have complex structures, diverse styles and contents, and become geometrically distorted or even curved during an image capturing process.

<h2>Layout Analysis</h2>

</div>